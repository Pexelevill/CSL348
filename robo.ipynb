{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPK/AYwxmXNjQr3FgxclKu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pexelevill/CSL348/blob/main/robo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjVOZh2igyO",
        "outputId": "70be68c3-c700-42ef-9928-b3795b1b2292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Reward Values: [0.45555556 0.16666667 0.4        0.66940211 0.05263158]\n",
            "Total Reward: 624\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "reward_history = []\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    reward_history.append(reward)\n",
        "\n",
        "print(\"Estimated Reward Values:\", estimates)\n",
        "\n",
        "total_reward = np.sum(reward_history)\n",
        "print(\"Total Reward:\", total_reward)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "cumulative_reward = 0\n",
        "cumulative_mean_rewards = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    cumulative_reward += reward\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "print(\"Cumulative Mean Rewards:\", cumulative_mean_rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FKF_m3ki2JU",
        "outputId": "4f3257e6-18a0-4199-9062-85d5e536c260"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Mean Rewards: [0.0, 0.5, 0.6666666666666666, 0.75, 0.8, 0.6666666666666666, 0.5714285714285714, 0.625, 0.6666666666666666, 0.6, 0.5454545454545454, 0.5, 0.46153846153846156, 0.5, 0.5333333333333333, 0.5625, 0.5294117647058824, 0.5, 0.5263157894736842, 0.5, 0.5238095238095238, 0.5454545454545454, 0.5652173913043478, 0.5833333333333334, 0.6, 0.6153846153846154, 0.5925925925925926, 0.6071428571428571, 0.6206896551724138, 0.6333333333333333, 0.6129032258064516, 0.59375, 0.5757575757575758, 0.5882352941176471, 0.5714285714285714, 0.5555555555555556, 0.5675675675675675, 0.5789473684210527, 0.5897435897435898, 0.575, 0.5609756097560976, 0.5714285714285714, 0.5581395348837209, 0.5681818181818182, 0.5555555555555556, 0.5434782608695652, 0.5319148936170213, 0.5416666666666666, 0.5510204081632653, 0.54, 0.5294117647058824, 0.5384615384615384, 0.5471698113207547, 0.5370370370370371, 0.5454545454545454, 0.5357142857142857, 0.543859649122807, 0.5517241379310345, 0.559322033898305, 0.5666666666666667, 0.5737704918032787, 0.5645161290322581, 0.5714285714285714, 0.578125, 0.5846153846153846, 0.5909090909090909, 0.5970149253731343, 0.5882352941176471, 0.5797101449275363, 0.5857142857142857, 0.5915492957746479, 0.5833333333333334, 0.589041095890411, 0.581081081081081, 0.5866666666666667, 0.5921052631578947, 0.5844155844155844, 0.5897435897435898, 0.5949367088607594, 0.5875, 0.5802469135802469, 0.573170731707317, 0.5783132530120482, 0.5833333333333334, 0.5764705882352941, 0.5697674418604651, 0.5747126436781609, 0.5795454545454546, 0.5842696629213483, 0.5777777777777777, 0.5824175824175825, 0.5869565217391305, 0.5913978494623656, 0.5851063829787234, 0.5789473684210527, 0.5833333333333334, 0.5876288659793815, 0.5816326530612245, 0.5858585858585859, 0.59, 0.594059405940594, 0.5980392156862745, 0.6019417475728155, 0.6057692307692307, 0.6, 0.5943396226415094, 0.5981308411214953, 0.6018518518518519, 0.5963302752293578, 0.5909090909090909, 0.5945945945945946, 0.5982142857142857, 0.5929203539823009, 0.5964912280701754, 0.591304347826087, 0.5862068965517241, 0.5897435897435898, 0.5932203389830508, 0.5966386554621849, 0.6, 0.5950413223140496, 0.5983606557377049, 0.6016260162601627, 0.6048387096774194, 0.608, 0.6111111111111112, 0.6141732283464567, 0.6171875, 0.6201550387596899, 0.6230769230769231, 0.6183206106870229, 0.6136363636363636, 0.6090225563909775, 0.6044776119402985, 0.6074074074074074, 0.6102941176470589, 0.6131386861313869, 0.6159420289855072, 0.6187050359712231, 0.6142857142857143, 0.6170212765957447, 0.6197183098591549, 0.6223776223776224, 0.6180555555555556, 0.6206896551724138, 0.6232876712328768, 0.6258503401360545, 0.6283783783783784, 0.6308724832214765, 0.6333333333333333, 0.6357615894039735, 0.631578947368421, 0.6274509803921569, 0.6298701298701299, 0.632258064516129, 0.6282051282051282, 0.6242038216560509, 0.6265822784810127, 0.6226415094339622, 0.61875, 0.6211180124223602, 0.6172839506172839, 0.6134969325153374, 0.6158536585365854, 0.6181818181818182, 0.6204819277108434, 0.6227544910179641, 0.625, 0.621301775147929, 0.6235294117647059, 0.6257309941520468, 0.622093023255814, 0.6242774566473989, 0.6264367816091954, 0.6285714285714286, 0.625, 0.6271186440677966, 0.6292134831460674, 0.6256983240223464, 0.6277777777777778, 0.6243093922651933, 0.6208791208791209, 0.6229508196721312, 0.625, 0.6270270270270271, 0.6290322580645161, 0.6310160427807486, 0.6329787234042553, 0.6349206349206349, 0.6368421052631579, 0.6387434554973822, 0.6354166666666666, 0.6373056994818653, 0.634020618556701, 0.6358974358974359, 0.6326530612244898, 0.6294416243654822, 0.6262626262626263, 0.628140703517588, 0.63, 0.6318407960199005, 0.6336633663366337, 0.6354679802955665, 0.6372549019607843, 0.6390243902439025, 0.6407766990291263, 0.642512077294686, 0.6394230769230769, 0.6411483253588517, 0.6428571428571429, 0.6445497630331753, 0.6462264150943396, 0.647887323943662, 0.6495327102803738, 0.6511627906976745, 0.6527777777777778, 0.6543778801843319, 0.6559633027522935, 0.6529680365296804, 0.6545454545454545, 0.6515837104072398, 0.6531531531531531, 0.6547085201793722, 0.65625, 0.6533333333333333, 0.6548672566371682, 0.6519823788546255, 0.6535087719298246, 0.6506550218340611, 0.6521739130434783, 0.6493506493506493, 0.6508620689655172, 0.648068669527897, 0.6495726495726496, 0.6468085106382979, 0.6440677966101694, 0.6413502109704642, 0.6428571428571429, 0.6443514644351465, 0.6416666666666667, 0.6431535269709544, 0.6446280991735537, 0.6460905349794238, 0.6434426229508197, 0.6408163265306123, 0.6382113821138211, 0.6356275303643725, 0.6370967741935484, 0.6345381526104418, 0.632, 0.6334661354581673, 0.6349206349206349, 0.6324110671936759, 0.6299212598425197, 0.6313725490196078, 0.6328125, 0.6342412451361867, 0.6356589147286822, 0.637065637065637, 0.6384615384615384, 0.6360153256704981, 0.6374045801526718, 0.6387832699619772, 0.6363636363636364, 0.6339622641509434, 0.6353383458646616, 0.6329588014981273, 0.6343283582089553, 0.6319702602230484, 0.6333333333333333, 0.6346863468634686, 0.6360294117647058, 0.6373626373626373, 0.6386861313868614, 0.6363636363636364, 0.6376811594202898, 0.6353790613718412, 0.6366906474820144, 0.6379928315412187, 0.6392857142857142, 0.6370106761565836, 0.6382978723404256, 0.6395759717314488, 0.6408450704225352, 0.6385964912280702, 0.6363636363636364, 0.6376306620209059, 0.6388888888888888, 0.6401384083044983, 0.6379310344827587, 0.6391752577319587, 0.636986301369863, 0.6348122866894198, 0.6360544217687075, 0.6372881355932203, 0.6385135135135135, 0.6397306397306397, 0.6409395973154363, 0.6421404682274248, 0.64, 0.6411960132890365, 0.6423841059602649, 0.6435643564356436, 0.6447368421052632, 0.6459016393442623, 0.6470588235294118, 0.6449511400651465, 0.6428571428571429, 0.6440129449838188, 0.6451612903225806, 0.6463022508038585, 0.6442307692307693, 0.645367412140575, 0.6464968152866242, 0.6476190476190476, 0.6487341772151899, 0.6466876971608833, 0.6446540880503144, 0.6426332288401254, 0.64375, 0.6448598130841121, 0.6459627329192547, 0.6470588235294118, 0.6481481481481481, 0.6492307692307693, 0.6503067484662577, 0.6513761467889908, 0.649390243902439, 0.6504559270516718, 0.6484848484848484, 0.6465256797583081, 0.6445783132530121, 0.6456456456456456, 0.6467065868263473, 0.6447761194029851, 0.6458333333333334, 0.6439169139465876, 0.6449704142011834, 0.6430678466076696, 0.6411764705882353, 0.6422287390029325, 0.6432748538011696, 0.6443148688046647, 0.6453488372093024, 0.6463768115942029, 0.6473988439306358, 0.6484149855907781, 0.6494252873563219, 0.6504297994269341, 0.6514285714285715, 0.6524216524216524, 0.6534090909090909, 0.6543909348441926, 0.652542372881356, 0.6535211267605634, 0.6544943820224719, 0.6526610644257703, 0.6508379888268156, 0.6518105849582173, 0.6527777777777778, 0.6537396121883656, 0.6519337016574586, 0.650137741046832, 0.6483516483516484, 0.6493150684931507, 0.6502732240437158, 0.6512261580381471, 0.6494565217391305, 0.6504065040650406, 0.6486486486486487, 0.6469002695417789, 0.6451612903225806, 0.6434316353887399, 0.6417112299465241, 0.64, 0.6409574468085106, 0.6419098143236074, 0.6428571428571429, 0.6411609498680739, 0.6421052631578947, 0.6430446194225722, 0.643979057591623, 0.6422976501305483, 0.640625, 0.6415584415584416, 0.6398963730569949, 0.6408268733850129, 0.6417525773195877, 0.6426735218508998, 0.6435897435897436, 0.6445012787723785, 0.6454081632653061, 0.6463104325699746, 0.6472081218274112, 0.6481012658227848, 0.648989898989899, 0.6498740554156172, 0.6482412060301508, 0.6466165413533834, 0.645, 0.6458852867830424, 0.6442786069651741, 0.6451612903225806, 0.6460396039603961, 0.6469135802469136, 0.6477832512315271, 0.6486486486486487, 0.6470588235294118, 0.6479217603911981, 0.6487804878048781, 0.6496350364963503, 0.6480582524271845, 0.648910411622276, 0.6473429951690821, 0.6481927710843374, 0.6466346153846154, 0.645083932853717, 0.645933014354067, 0.6443914081145584, 0.6452380952380953, 0.6460807600950119, 0.6445497630331753, 0.6430260047281324, 0.6415094339622641, 0.6423529411764706, 0.6431924882629108, 0.6440281030444965, 0.6425233644859814, 0.6433566433566433, 0.6441860465116279, 0.6450116009280742, 0.6435185185185185, 0.6443418013856813, 0.6428571428571429, 0.6436781609195402, 0.6444954128440367, 0.6453089244851259, 0.6461187214611872, 0.6469248291571754, 0.6477272727272727, 0.6485260770975056, 0.6470588235294118, 0.6455981941309256, 0.6463963963963963, 0.647191011235955, 0.647982062780269, 0.6465324384787472, 0.6473214285714286, 0.6481069042316259, 0.6466666666666666, 0.647450110864745, 0.6482300884955752, 0.6467991169977925, 0.6475770925110133, 0.6483516483516484, 0.6491228070175439, 0.649890590809628, 0.648471615720524, 0.6492374727668845, 0.65, 0.6507592190889371, 0.6493506493506493, 0.6501079913606912, 0.6487068965517241, 0.6473118279569893, 0.648068669527897, 0.6488222698072805, 0.6495726495726496, 0.650319829424307, 0.648936170212766, 0.6475583864118896, 0.6483050847457628, 0.6490486257928119, 0.6476793248945147, 0.6484210526315789, 0.6491596638655462, 0.649895178197065, 0.6506276150627615, 0.6492693110647182, 0.65, 0.6507276507276507, 0.6514522821576764, 0.6521739130434783, 0.6508264462809917, 0.6515463917525773, 0.6502057613168725, 0.648870636550308, 0.6495901639344263, 0.6503067484662577, 0.6489795918367347, 0.6496945010183299, 0.6504065040650406, 0.6490872210953347, 0.6477732793522267, 0.6484848484848484, 0.6491935483870968, 0.647887323943662, 0.6485943775100401, 0.6472945891783567, 0.648, 0.6487025948103793, 0.649402390438247, 0.6481113320079522, 0.6468253968253969, 0.6475247524752475, 0.6462450592885376, 0.6449704142011834, 0.6456692913385826, 0.6463654223968566, 0.6470588235294118, 0.6477495107632094, 0.6484375, 0.6471734892787524, 0.6459143968871596, 0.6466019417475728, 0.6453488372093024, 0.6460348162475822, 0.6467181467181468, 0.6473988439306358, 0.6480769230769231, 0.6487523992322457, 0.6494252873563219, 0.6500956022944551, 0.648854961832061, 0.6495238095238095, 0.6501901140684411, 0.650853889943074, 0.6515151515151515, 0.6521739130434783, 0.6509433962264151, 0.6516007532956686, 0.6522556390977443, 0.6529080675422139, 0.653558052434457, 0.6542056074766355, 0.6548507462686567, 0.6536312849162011, 0.654275092936803, 0.6549165120593692, 0.6537037037037037, 0.6543438077634011, 0.6549815498154982, 0.6556169429097606, 0.65625, 0.6568807339449542, 0.6575091575091575, 0.6581352833638026, 0.6587591240875912, 0.6593806921675774, 0.66, 0.6606170598911071, 0.6612318840579711, 0.6618444846292948, 0.6606498194945848, 0.6594594594594595, 0.6600719424460432, 0.6606822262118492, 0.6612903225806451, 0.6618962432915921, 0.6625, 0.661319073083779, 0.6619217081850534, 0.6607460035523979, 0.6613475177304965, 0.6619469026548672, 0.6625441696113075, 0.6631393298059964, 0.6619718309859155, 0.6608084358523726, 0.6596491228070176, 0.660245183887916, 0.6608391608391608, 0.6614310645724258, 0.662020905923345, 0.6608695652173913, 0.6614583333333334, 0.6603119584055459, 0.6608996539792388, 0.6614853195164075, 0.6603448275862069, 0.6609294320137694, 0.6597938144329897, 0.660377358490566, 0.660958904109589, 0.6598290598290598, 0.658703071672355, 0.6592844974446337, 0.6598639455782312, 0.6587436332767402, 0.6593220338983051, 0.6598984771573604, 0.660472972972973, 0.6610455311973018, 0.6616161616161617, 0.6605042016806723, 0.6610738255033557, 0.661641541038526, 0.6622073578595318, 0.66110183639399, 0.66, 0.6589018302828619, 0.659468438538206, 0.6583747927031509, 0.6589403973509934, 0.6595041322314049, 0.6600660066006601, 0.6606260296540363, 0.6611842105263158, 0.6617405582922824, 0.6622950819672131, 0.662847790507365, 0.6633986928104575, 0.6639477977161501, 0.6628664495114006, 0.6634146341463415, 0.663961038961039, 0.6645056726094003, 0.6634304207119741, 0.6623586429725363, 0.6629032258064517, 0.6634460547504025, 0.6639871382636656, 0.6645264847512039, 0.6650641025641025, 0.664, 0.6629392971246006, 0.6618819776714514, 0.6624203821656051, 0.6613672496025437, 0.6603174603174603, 0.6608557844690967, 0.6613924050632911, 0.6619273301737757, 0.6608832807570978, 0.6614173228346457, 0.660377358490566, 0.6609105180533752, 0.6614420062695925, 0.6619718309859155, 0.6609375, 0.6599063962558502, 0.660436137071651, 0.6594090202177294, 0.6599378881987578, 0.6604651162790698, 0.6594427244582043, 0.6584234930448223, 0.6589506172839507, 0.6594761171032357, 0.66, 0.6605222734254992, 0.6595092024539877, 0.6600306278713629, 0.6605504587155964, 0.6610687022900763, 0.6615853658536586, 0.6621004566210046, 0.662613981762918, 0.6631259484066768, 0.6636363636363637, 0.6641452344931922, 0.6646525679758308, 0.665158371040724, 0.6656626506024096, 0.6646616541353384, 0.6651651651651652, 0.6656671664167916, 0.6661676646706587, 0.6666666666666666, 0.6671641791044776, 0.6676602086438153, 0.6666666666666666, 0.6671619613670133, 0.6676557863501483, 0.6681481481481482, 0.6671597633136095, 0.6676514032496307, 0.668141592920354, 0.6671575846833578, 0.6661764705882353, 0.6666666666666666, 0.6671554252199413, 0.6676427525622255, 0.6681286549707602, 0.6686131386861314, 0.6690962099125365, 0.6681222707423581, 0.6686046511627907, 0.6690856313497823, 0.6695652173913044, 0.6700434153400868, 0.6705202312138728, 0.670995670995671, 0.670028818443804, 0.6690647482014388, 0.6695402298850575, 0.6685796269727403, 0.669054441260745, 0.6695278969957081, 0.6685714285714286, 0.6690442225392297, 0.6680911680911681, 0.6685633001422475, 0.6690340909090909, 0.6680851063829787, 0.6685552407932012, 0.669024045261669, 0.6680790960451978, 0.6671368124118476, 0.6676056338028169, 0.6680731364275668, 0.6671348314606742, 0.667601683029453, 0.6680672268907563, 0.6685314685314685, 0.6689944134078212, 0.6680613668061367, 0.6671309192200557, 0.6662030598052852, 0.6652777777777777, 0.6643550624133149, 0.6648199445983379, 0.665283540802213, 0.6657458563535912, 0.6662068965517242, 0.6652892561983471, 0.6657496561210454, 0.6662087912087912, 0.6666666666666666, 0.6671232876712329, 0.667578659370725, 0.6680327868852459, 0.6671214188267395, 0.667574931880109, 0.6680272108843538, 0.6671195652173914, 0.6675712347354138, 0.6680216802168022, 0.6671177266576455, 0.6675675675675675, 0.6666666666666666, 0.6671159029649596, 0.6675639300134589, 0.668010752688172, 0.6684563758389261, 0.6675603217158177, 0.6666666666666666, 0.6657754010695187, 0.664886515353805, 0.6653333333333333, 0.6644474034620506, 0.6648936170212766, 0.6653386454183267, 0.6657824933687002, 0.6662251655629139, 0.6653439153439153, 0.665785997357992, 0.6662269129287599, 0.6666666666666666, 0.6671052631578948, 0.6675427069645203, 0.6679790026246719, 0.6671035386631717, 0.6662303664921466, 0.6666666666666666, 0.6657963446475196, 0.666232073011734, 0.6653645833333334, 0.6644993498049415, 0.6636363636363637, 0.6640726329442282, 0.6632124352331606, 0.6636481241914618, 0.6627906976744186, 0.6632258064516129, 0.663659793814433, 0.6628056628056628, 0.6619537275064268, 0.6623876765083441, 0.6615384615384615, 0.6619718309859155, 0.6611253196930946, 0.6602809706257982, 0.6607142857142857, 0.6598726114649681, 0.6603053435114504, 0.6607369758576874, 0.6598984771573604, 0.6590621039290241, 0.6594936708860759, 0.6599241466498104, 0.6603535353535354, 0.6595208070617906, 0.6586901763224181, 0.6591194968553459, 0.6582914572864321, 0.6587202007528231, 0.6591478696741855, 0.6583229036295369, 0.6575, 0.6579275905118602, 0.6583541147132169, 0.6587795765877957, 0.6592039800995025, 0.6596273291925466, 0.6588089330024814, 0.6579925650557621, 0.6584158415841584, 0.6588380716934487, 0.6580246913580247, 0.6584463625154131, 0.6576354679802956, 0.6568265682656826, 0.6560196560196561, 0.656441717791411, 0.6568627450980392, 0.6572827417380661, 0.6577017114914425, 0.6581196581196581, 0.6585365853658537, 0.658952496954933, 0.6593673965936739, 0.6597812879708383, 0.6601941747572816, 0.6606060606060606, 0.6610169491525424, 0.660217654171705, 0.6606280193236715, 0.6598311218335344, 0.6602409638554216, 0.6594464500601684, 0.6586538461538461, 0.65906362545018, 0.658273381294964, 0.6574850299401198, 0.6578947368421053, 0.6583034647550776, 0.6575178997613366, 0.6579261025029798, 0.6583333333333333, 0.6587395957193817, 0.6579572446555819, 0.6583629893238434, 0.6587677725118484, 0.6579881656804734, 0.6572104018912529, 0.6576151121605667, 0.6580188679245284, 0.657243816254417, 0.6576470588235294, 0.6580493537015276, 0.6584507042253521, 0.6576787807737398, 0.6569086651053864, 0.656140350877193, 0.6565420560747663, 0.6557759626604434, 0.6561771561771562, 0.6565774155995343, 0.6569767441860465, 0.6573751451800233, 0.6566125290023201, 0.657010428736964, 0.6574074074074074, 0.6578034682080924, 0.6570438799076213, 0.657439446366782, 0.6578341013824884, 0.6582278481012658, 0.6574712643678161, 0.6578645235361653, 0.6571100917431193, 0.6575028636884307, 0.6578947368421053, 0.6582857142857143, 0.658675799086758, 0.6579247434435576, 0.6583143507972665, 0.658703071672355, 0.6590909090909091, 0.659477866061294, 0.6598639455782312, 0.6591166477916195, 0.6595022624434389, 0.6598870056497175, 0.6602708803611738, 0.6606538895152199, 0.6599099099099099, 0.6602924634420697, 0.6606741573033708, 0.6610549943883277, 0.6614349775784754, 0.6606942889137738, 0.6599552572706935, 0.6603351955307263, 0.6595982142857143, 0.6599777034559643, 0.6603563474387528, 0.6596218020022246, 0.66, 0.660377358490566, 0.6607538802660754, 0.6611295681063123, 0.6603982300884956, 0.6607734806629835, 0.6600441501103753, 0.6604189636163176, 0.6596916299559471, 0.6600660066006601, 0.6604395604395604, 0.6597145993413831, 0.6589912280701754, 0.6582694414019715, 0.6575492341356673, 0.6568306010928961, 0.6572052401746725, 0.6575790621592148, 0.6568627450980392, 0.6572361262241567, 0.657608695652174, 0.6568946796959826, 0.6561822125813449, 0.656554712892741, 0.6558441558441559, 0.6551351351351351, 0.6544276457883369, 0.6537216828478964, 0.6540948275862069, 0.6544671689989235, 0.6548387096774193, 0.6552094522019334, 0.6545064377682404, 0.654876741693462, 0.6541755888650964, 0.653475935828877, 0.6527777777777778, 0.6531483457844184, 0.652452025586354, 0.6517571884984026, 0.6510638297872341, 0.6503719447396387, 0.6507430997876857, 0.6511134676564156, 0.6514830508474576, 0.6507936507936508, 0.6511627906976745, 0.6515311510031679, 0.6508438818565401, 0.6512118018967334, 0.651578947368421, 0.650893796004206, 0.6512605042016807, 0.6516264428121721, 0.6509433962264151, 0.6513089005235602, 0.6516736401673641, 0.6520376175548589, 0.6524008350730689, 0.6527632950990615, 0.653125, 0.6524453694068678, 0.6528066528066528, 0.6531671858774662, 0.6524896265560166, 0.6528497409326425, 0.6532091097308489, 0.6525336091003102, 0.6518595041322314, 0.6522187822497421, 0.6525773195876289, 0.6519052523171988, 0.6512345679012346, 0.6515930113052415, 0.6519507186858317, 0.6523076923076923, 0.6516393442622951, 0.6519959058341863, 0.6523517382413088, 0.651685393258427, 0.6510204081632653, 0.6513761467889908, 0.6507128309572301, 0.6510681586978637, 0.6514227642276422, 0.6517766497461929, 0.652129817444219, 0.6524822695035462, 0.652834008097166, 0.6531850353892821, 0.6535353535353535, 0.6538849646821393, 0.6542338709677419, 0.6545820745216515, 0.6549295774647887, 0.6552763819095477, 0.6556224899598394, 0.6559679037111334, 0.656312625250501, 0.6556556556556556, 0.656]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [0.4, 0.3, 0.5, 0.7, 0.2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "total_rewards_per_trial = []\n",
        "cumulative_mean_rewards_per_trial = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "    cumulative_mean_rewards = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "        cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    cumulative_mean_rewards_per_trial.append(cumulative_mean_rewards)\n",
        "\n",
        "average_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "# Calculate the average cumulative mean rewards after all trials\n",
        "average_cumulative_mean_rewards = np.mean(cumulative_mean_rewards_per_trial, axis=0)\n",
        "\n",
        "print(\"Average Total Reward across Trials:\", average_total_reward)\n",
        "print(\"Average Cumulative Mean Rewards across Trials:\", average_cumulative_mean_rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6NkyaZPi_El",
        "outputId": "c403e153-b159-41b4-dd41-703241c6b041"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Total Reward across Trials: 420.221\n",
            "Average Cumulative Mean Rewards across Trials: [0.423      0.415      0.41666667 0.421      0.4216     0.42216667\n",
            " 0.42514286 0.427125   0.42766667 0.4306     0.43090909 0.42941667\n",
            " 0.42938462 0.4265     0.4246     0.4244375  0.42452941 0.42566667\n",
            " 0.426      0.4259     0.42609524 0.42468182 0.42473913 0.42470833\n",
            " 0.42408    0.42461538 0.425      0.42446429 0.42434483 0.4241\n",
            " 0.42345161 0.42346875 0.42418182 0.42364706 0.42328571 0.42266667\n",
            " 0.42327027 0.42352632 0.42371795 0.42405    0.42378049 0.42378571\n",
            " 0.42465116 0.42461364 0.42395556 0.424      0.42393617 0.42422917\n",
            " 0.42453061 0.42398    0.424      0.42384615 0.42335849 0.42275926\n",
            " 0.4222     0.42198214 0.42205263 0.42168966 0.42174576 0.42161667\n",
            " 0.42162295 0.42153226 0.42122222 0.42121875 0.42129231 0.42134848\n",
            " 0.42138806 0.4215     0.42162319 0.42202857 0.42212676 0.42186111\n",
            " 0.42146575 0.42125676 0.42142667 0.42131579 0.42125974 0.42147436\n",
            " 0.42139241 0.4212     0.42132099 0.42110976 0.42139759 0.42127381\n",
            " 0.42124706 0.42102326 0.42103448 0.42086364 0.42124719 0.42121111\n",
            " 0.42125275 0.42138043 0.42109677 0.42106383 0.42083158 0.42097917\n",
            " 0.42080412 0.42083673 0.42084848 0.42092    0.42093069 0.42101961\n",
            " 0.42108738 0.42113462 0.42108571 0.42103774 0.42104673 0.42114815\n",
            " 0.42118349 0.42152727 0.42148649 0.42142857 0.42132743 0.4212807\n",
            " 0.42141739 0.42136207 0.42119658 0.42115254 0.42101681 0.42083333\n",
            " 0.4208595  0.42077869 0.42079675 0.42087903 0.420864   0.42074603\n",
            " 0.42072441 0.42092188 0.42097674 0.42090769 0.4210687  0.42102273\n",
            " 0.42090977 0.42086567 0.42088889 0.42091176 0.42067153 0.42057971\n",
            " 0.42051799 0.42049286 0.42043972 0.42046479 0.42041958 0.42013889\n",
            " 0.42031034 0.42034932 0.4204966  0.42026351 0.42032215 0.42038667\n",
            " 0.42046358 0.42044079 0.42052288 0.42062987 0.42044516 0.4204359\n",
            " 0.4203949  0.42026582 0.42014465 0.42018125 0.42031677 0.42024691\n",
            " 0.42011656 0.42007317 0.42001818 0.42003614 0.42000599 0.41998214\n",
            " 0.42       0.42011765 0.42009357 0.42013372 0.42010983 0.42016667\n",
            " 0.42014857 0.42004545 0.4200452  0.42004494 0.42010056 0.42005\n",
            " 0.4200884  0.42013187 0.42008197 0.42008696 0.42008108 0.41996774\n",
            " 0.42013904 0.42014362 0.42020635 0.42014737 0.42017801 0.42009896\n",
            " 0.41990155 0.42001031 0.42006667 0.42005612 0.41998477 0.42013131\n",
            " 0.42005025 0.420115   0.42003483 0.42016337 0.42016256 0.42013235\n",
            " 0.42016585 0.42015049 0.42028019 0.42024038 0.42025359 0.42008571\n",
            " 0.42016588 0.42019811 0.42020188 0.42010748 0.42017209 0.42022685\n",
            " 0.42024885 0.42023394 0.42025571 0.42016818 0.42015837 0.42016216\n",
            " 0.42021973 0.42030357 0.42040444 0.42045133 0.42042291 0.42040351\n",
            " 0.42047598 0.42041304 0.420329   0.42026724 0.42024034 0.42029487\n",
            " 0.42027234 0.42030508 0.42036709 0.42023109 0.42023013 0.4202125\n",
            " 0.42014108 0.42015289 0.42018519 0.42018443 0.42021224 0.42022358\n",
            " 0.42025101 0.42029839 0.42035743 0.42048    0.42041036 0.42030556\n",
            " 0.42019763 0.42022441 0.42023137 0.42019922 0.42020233 0.42022868\n",
            " 0.42028571 0.42029615 0.42029502 0.42037405 0.42038783 0.42041288\n",
            " 0.42036604 0.42042481 0.42041948 0.4203806  0.42034944 0.42032963\n",
            " 0.42032472 0.42037132 0.42036996 0.42023358 0.42030545 0.42035145\n",
            " 0.4203935  0.4204964  0.42055914 0.42055    0.42042705 0.42042908\n",
            " 0.42039576 0.42042958 0.42051228 0.42052448 0.42043902 0.42048611\n",
            " 0.42051557 0.42048621 0.4204433  0.42039384 0.42037543 0.42036395\n",
            " 0.42037288 0.42044257 0.42038721 0.42041275 0.42033445 0.42034\n",
            " 0.42037209 0.42048013 0.42045545 0.42033553 0.42028197 0.42028431\n",
            " 0.42028339 0.42032143 0.42034628 0.4202871  0.42026045 0.42021474\n",
            " 0.42019169 0.42024204 0.42025397 0.42024684 0.42021136 0.4202673\n",
            " 0.42019749 0.42011562 0.42009034 0.41995963 0.41998452 0.41992593\n",
            " 0.41992923 0.41989264 0.41988073 0.41990244 0.41989666 0.41987879\n",
            " 0.41989124 0.41994578 0.41992492 0.4199012  0.41996418 0.42000298\n",
            " 0.41996736 0.41995858 0.4199882  0.41992941 0.41990616 0.41981871\n",
            " 0.4198484  0.41977616 0.41977971 0.41966474 0.41976081 0.41975287\n",
            " 0.41972493 0.41968286 0.41966097 0.4196733  0.41968839 0.41966667\n",
            " 0.4196338  0.41967697 0.41973109 0.41970112 0.41969916 0.41964722\n",
            " 0.41963158 0.41966022 0.41967493 0.41967582 0.41964932 0.41959016\n",
            " 0.41965123 0.4196712  0.41963144 0.41965676 0.41966038 0.41966667\n",
            " 0.41970777 0.41978075 0.419832   0.41985372 0.41984085 0.41983598\n",
            " 0.41986807 0.41988421 0.41991076 0.41996335 0.41992689 0.41996094\n",
            " 0.41991688 0.41990415 0.41995607 0.41991237 0.41988689 0.41985641\n",
            " 0.41978261 0.41977806 0.41981679 0.41981218 0.41978228 0.41980808\n",
            " 0.4197733  0.41973618 0.41974687 0.4196875  0.4196808  0.41966915\n",
            " 0.41967742 0.41960891 0.41957284 0.41956897 0.41961425 0.41960784\n",
            " 0.41956235 0.41957561 0.41955718 0.41955097 0.41956659 0.4195942\n",
            " 0.41962651 0.41964663 0.41963549 0.41965072 0.41968496 0.41964286\n",
            " 0.41970071 0.4197109  0.41973995 0.4197217  0.41969882 0.41970423\n",
            " 0.41976815 0.41976402 0.41982051 0.41983488 0.41981671 0.41985185\n",
            " 0.41985912 0.41989631 0.41994253 0.41994266 0.41993135 0.41993379\n",
            " 0.41993622 0.42000682 0.41996372 0.41998643 0.4199526  0.41991441\n",
            " 0.41988764 0.41990583 0.41990157 0.419875   0.41985301 0.41986222\n",
            " 0.41986475 0.41992035 0.41996468 0.41985903 0.41989231 0.41985088\n",
            " 0.41981182 0.41983624 0.41982571 0.41988913 0.41984816 0.419829\n",
            " 0.41981641 0.41980172 0.41975484 0.41975322 0.41975589 0.41972863\n",
            " 0.41974414 0.41979574 0.41980467 0.41980932 0.41984567 0.41984388\n",
            " 0.41984632 0.41981303 0.41984277 0.41990586 0.41989144 0.41992708\n",
            " 0.41996258 0.41996888 0.41995238 0.41994835 0.41991753 0.41986626\n",
            " 0.41985216 0.4198832  0.41995501 0.41992041 0.41995519 0.41999797\n",
            " 0.42001623 0.42001012 0.41994747 0.41989516 0.41988732 0.41990763\n",
            " 0.41987976 0.419894   0.41989421 0.41993625 0.41994433 0.41992063\n",
            " 0.41990495 0.41992688 0.41994477 0.41999016 0.42003536 0.4200549\n",
            " 0.42003718 0.42011328 0.42009552 0.42009728 0.42014175 0.42017829\n",
            " 0.4201528  0.4201583  0.42013102 0.42014808 0.42011132 0.42013027\n",
            " 0.42013384 0.42016031 0.42012571 0.42013308 0.42016698 0.42018182\n",
            " 0.42017958 0.42019811 0.42020151 0.42022744 0.4201651  0.42013483\n",
            " 0.42012336 0.42016604 0.42013408 0.42008922 0.42011132 0.42015185\n",
            " 0.42014233 0.42018081 0.42016022 0.42022426 0.42024037 0.4202326\n",
            " 0.42021207 0.42020073 0.42020036 0.42021091 0.42024682 0.42025725\n",
            " 0.42026944 0.42024188 0.42027027 0.42031295 0.42031059 0.42031004\n",
            " 0.42028265 0.42028214 0.42025134 0.42024021 0.42021137 0.42023759\n",
            " 0.42026372 0.42026678 0.42031746 0.42032746 0.42033216 0.42030702\n",
            " 0.4202662  0.42028846 0.42026178 0.42025784 0.42023652 0.42022222\n",
            " 0.4202305  0.42020761 0.42018653 0.4201931  0.42023408 0.42024399\n",
            " 0.42024185 0.42025342 0.42028547 0.42025768 0.42024702 0.4202381\n",
            " 0.42022411 0.42020339 0.42020305 0.42019932 0.42020573 0.4202138\n",
            " 0.42018151 0.42019631 0.4201876  0.42020736 0.42023372 0.42025333\n",
            " 0.42024792 0.42023588 0.42021725 0.42018543 0.42016198 0.42017987\n",
            " 0.42023723 0.42022368 0.42023317 0.42024754 0.42025041 0.42030065\n",
            " 0.42029201 0.42027687 0.42027805 0.42031494 0.42037439 0.4203754\n",
            " 0.42039257 0.42042581 0.42046055 0.42046785 0.42047994 0.42050481\n",
            " 0.420536   0.42053674 0.42057097 0.42055573 0.42057393 0.42056508\n",
            " 0.42057686 0.42057595 0.42059874 0.42054574 0.42050551 0.42042767\n",
            " 0.42044113 0.42047806 0.42044131 0.42048437 0.4204961  0.42050935\n",
            " 0.42050389 0.42049534 0.42048372 0.42047988 0.42045131 0.42045988\n",
            " 0.4204869  0.42049538 0.42048387 0.42045706 0.42044717 0.42040826\n",
            " 0.42043359 0.42044817 0.42047489 0.42047416 0.42052504 0.4205303\n",
            " 0.42051589 0.42050906 0.42051735 0.42055572 0.42055038 0.4205961\n",
            " 0.42051274 0.42056737 0.42056054 0.42054478 0.42056334 0.42054613\n",
            " 0.42060624 0.42062463 0.42061778 0.42064201 0.42063959 0.42067552\n",
            " 0.42069072 0.42069853 0.42069163 0.42068768 0.42068228 0.42070468\n",
            " 0.42069635 0.42065015 0.42062445 0.42062791 0.4206328  0.42063913\n",
            " 0.42061795 0.4206474  0.42061616 0.42063112 0.4206518  0.42065948\n",
            " 0.42067288 0.42069771 0.42065808 0.42071857 0.4206933  0.42066667\n",
            " 0.42064438 0.42062358 0.42060709 0.42061615 0.42059123 0.42061017\n",
            " 0.42060085 0.42063099 0.42065541 0.4206559  0.42059748 0.42063585\n",
            " 0.42061818 0.42063966 0.42065132 0.4206337  0.42060223 0.42056806\n",
            " 0.42052427 0.42052355 0.42048963 0.42049309 0.42049793 0.42053581\n",
            " 0.4205337  0.42053159 0.42053361 0.42050959 0.42047332 0.42047951\n",
            " 0.42048158 0.4204564  0.42046259 0.42044973 0.42041248 0.42038347\n",
            " 0.42039378 0.42040135 0.42039406 0.42041644 0.420393   0.42039247\n",
            " 0.42039329 0.42038606 0.4203494  0.42033021 0.42029239 0.42029733\n",
            " 0.4203036  0.42030585 0.42029216 0.42029178 0.42029536 0.42032937\n",
            " 0.42032629 0.42032718 0.4203083  0.42027895 0.42027858 0.42024934\n",
            " 0.42023853 0.42022251 0.42022745 0.42018016 0.4201734  0.42017578\n",
            " 0.42017685 0.4201974  0.42020623 0.42021762 0.4202251  0.42020801\n",
            " 0.42022323 0.42024227 0.42025225 0.42026478 0.42028755 0.42026795\n",
            " 0.42027657 0.42028517 0.42029374 0.42028189 0.42028025 0.4202799\n",
            " 0.42026557 0.42026904 0.4202763  0.4202443  0.4202402  0.42023864\n",
            " 0.42022699 0.42025693 0.42024277 0.42024246 0.42026098 0.42025439\n",
            " 0.42022778 0.42024125 0.42023221 0.42022818 0.42022416 0.42023383\n",
            " 0.42024845 0.42023573 0.42023544 0.42022772 0.42023857 0.42021852\n",
            " 0.42019359 0.42025    0.42025461 0.42023587 0.42024785 0.42024387\n",
            " 0.42020318 0.42022616 0.4201917  0.42017683 0.42016809 0.4202129\n",
            " 0.42019441 0.42020388 0.42019152 0.42019855 0.42020556 0.42019928\n",
            " 0.42019059 0.42018193 0.42020096 0.42021995 0.4202473  0.42023141\n",
            " 0.42021916 0.4201866  0.42018638 0.42016348 0.4201621  0.42014405\n",
            " 0.4201415  0.42016627 0.42019454 0.42019905 0.42019645 0.42021513\n",
            " 0.4202255  0.42021816 0.42021084 0.42018588 0.42017039 0.42018897\n",
            " 0.42018288 0.42016511 0.4201614  0.42014953 0.42010968 0.42011422\n",
            " 0.42010827 0.42014535 0.42015563 0.42016125 0.42013789 0.4201412\n",
            " 0.42014913 0.42014781 0.42016148 0.42017627 0.42016571 0.42016667\n",
            " 0.42017222 0.42015711 0.42016953 0.420127   0.42015086 0.42013356\n",
            " 0.42011631 0.42012301 0.42014448 0.42016477 0.42017026 0.42018027\n",
            " 0.42017667 0.42015271 0.42018192 0.42018059 0.42019166 0.4201768\n",
            " 0.42018785 0.42019551 0.42018631 0.42017713 0.42017805 0.42019016\n",
            " 0.42017207 0.42016853 0.42014381 0.42014254 0.42012458 0.42013556\n",
            " 0.4201232  0.42013082 0.42016168 0.4201781  0.42018232 0.42018543\n",
            " 0.42020287 0.42018722 0.42017382 0.42017582 0.42020198 0.42020395\n",
            " 0.42020044 0.42024726 0.4202306  0.42021397 0.42023882 0.4202244\n",
            " 0.42021763 0.42022174 0.4202367  0.42026356 0.4202286  0.42022727\n",
            " 0.42022486 0.42023758 0.42023301 0.42024569 0.42023036 0.42023441\n",
            " 0.42023201 0.42024249 0.42026902 0.42026445 0.42029305 0.42028953\n",
            " 0.42028602 0.42027399 0.42025666 0.42023723 0.42027311 0.42027707\n",
            " 0.42027996 0.42030191 0.42030476 0.42029387 0.42030517 0.42031435\n",
            " 0.42034984 0.42036316 0.42037119 0.42033508 0.4203085  0.42030189\n",
            " 0.4202733  0.42025314 0.42023929 0.420238   0.42023253 0.42025625\n",
            " 0.42027471 0.42026715 0.42026791 0.42025104 0.42026425 0.4202619\n",
            " 0.42024302 0.4202562  0.42026109 0.42026495 0.42025644 0.42024794\n",
            " 0.4202446  0.42024641 0.42022462 0.42021721 0.42020983 0.42021063\n",
            " 0.42019714 0.42021327 0.4201998  0.42019348 0.42021668 0.42023272\n",
            " 0.42020508 0.4202069  0.42022594 0.4202247  0.42022952 0.42021919\n",
            " 0.42023108 0.42020464 0.42020645 0.42020221 0.4202201  0.42023896\n",
            " 0.42021063 0.42019439 0.4201992  0.420221  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    def choose_arm(epsilon, estimates):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(num_arms)\n",
        "        else:\n",
        "            return np.argmax(estimates)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm:\", estimated_mean_rewards_per_arm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtosalrUjB90",
        "outputId": "10990a17-9f49-4a29-abc5-f05cc00e0de0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm: [0.36176635 0.27107547 0.45104853 0.6319367  0.17996477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXQ5W2S3jL6Q",
        "outputId": "b26a474a-a03e-43aa-c769-cba4a3403395"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (Greedy Approach): [0.36065507 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Greedy approach: Always choose the arm with the highest estimated mean reward\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLOhlDgejNV7",
        "outputId": "ee9b6c8c-4451-4ffa-aea1-4ee07f9ace5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (Greedy Approach): [0.36120852 0.         0.         0.         0.        ]\n",
            "Mean Total Reward across Trials: 571.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "\n",
        "            chosen_arm = np.random.choice(num_arms)\n",
        "        else:\n",
        "\n",
        "            chosen_arm = np.argmax(estimates)\n",
        "\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (ε-Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg1_L2NijZ6Z",
        "outputId": "31407f64-c9da-4b5a-d806-62fe7e8fe6f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (ε-Greedy Approach): [0.36227151 0.27083332 0.45087595 0.6319367  0.18099002]\n",
            "Mean Total Reward across Trials: 599.957\n"
          ]
        }
      ]
    }
  ]
}